{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks Summary\n",
    "\n",
    "* Different Types of Neurons\n",
    "* Calculating Error\n",
    "* Different NN architectures\n",
    "\n",
    "---\n",
    "## Different Types of Neurons\n",
    "\n",
    "### Linear\n",
    "\n",
    "Activation follows a linear function: \n",
    "\n",
    "$y = b(bias) + \\sum{x_i w_i}$\n",
    "\n",
    "### Sigmoid\n",
    "\n",
    "Activation follows sigmoid function:\n",
    "\n",
    "$z = b + \\sum{x_i * w_i}$\n",
    "\n",
    "$y = \\frac{1}{1 + e^{-z}}$\n",
    "\n",
    "### Binary Threshold\n",
    "\n",
    "Activation function is on or off. b can be negative such that:\n",
    "\n",
    "$z = b + \\sum{x_i * w_i}$\n",
    "\n",
    "$y =\n",
    "  \\begin{cases}\n",
    "    1       & \\quad \\text{if } z \\geq 0\\\\\n",
    "    0       & \\quad \\text{otherwise} \\\\\n",
    "  \\end{cases}\n",
    "$\n",
    "\n",
    "### Rectified Linear\n",
    "\n",
    "Activation has threshold and linear function beyond the threshold:\n",
    "\n",
    "$z = b + \\sum{x_i * w_i}$ (bias can be negative to elongate activation)\n",
    "\n",
    "$y =\n",
    "  \\begin{cases}\n",
    "    z       & \\quad \\text{if } z > 0\\\\\n",
    "    0       & \\quad \\text{otherwise} \\\\\n",
    "  \\end{cases}\n",
    "$\n",
    "\n",
    "### Stochastic Binary Neuron\n",
    "\n",
    "Activation is probability of producing activation:\n",
    "\n",
    "$p(s=1) = \\frac{1}{1 + e^{-z}}$\n",
    "\n",
    "---\n",
    "\n",
    "## Hyperparameter Tuning\n",
    "\n",
    "Meant to help generalize the model to future data. Usually used on the cross-validation set.\n",
    "\n",
    "Overfitting can also be avoided by:\n",
    "\n",
    "* weight decay\n",
    "* weight share\n",
    "* early stopping\n",
    "* model averaging\n",
    "* bayes fitting\n",
    "* drop out\n",
    "* generative pretraining\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "Calculate a loss function moving towards a global/local minumum. Run some data through the network, calculate loss, update weights and do it again. Calculating loss and updating weights _should_ help reduce the overall error.\n",
    "\n",
    "### Grid Search\n",
    "\n",
    "Given a set of hyperparameters, create a cartesian product of the params and run a model with each member of the set of the cartesian product.\n",
    "\n",
    "Ex:\n",
    "\n",
    "param a: {1,2,3}\n",
    "\n",
    "param b: {a,b,c}\n",
    "\n",
    "grid = {(1,a), (1,b) ... (3,c)}\n",
    "\n",
    "```\n",
    "for mem in grid:\n",
    "  run_model(mem)\n",
    "\n",
    "```\n",
    "\n",
    "## Calculating Error and Adjusting Weights\n",
    "\n",
    "### Linear Function\n",
    "\n",
    "[helpful source](http://sebastianraschka.com/Articles/2015_singlelayer_neurons.html#adaptive-linear-neurons-and-the-delta-rule)\n",
    "\n",
    "Calculates a linear loss on a continuous output.\n",
    "\n",
    "NOTE: target = the true class label\n",
    "\n",
    "Loss Function:\n",
    "\n",
    "$J(w) = \\frac{1}{2} \\sum{ (target^i - output^i)^2 }$\n",
    "\n",
    "Calculating Change in Weights:\n",
    "\n",
    "$\\Delta w_j = - \\epsilon \\frac{\\partial J}{\\partial w_j}$ where $\\epsilon$ is the learning rate and j/w_j is the partial derivative of loss funtion with respect to the changing weights\n",
    "\n",
    "$\\Delta w_j = \\epsilon \\sum{(t^i - o^i)x_j^i}$\n",
    "\n",
    "### Logistic Function\n",
    "\n",
    "[Stanford Lecture Notes](http://cs229.stanford.edu/notes/cs229-notes1.pdf) I find those to be more helpful than understanding the lecture notes for this section.\n",
    "\n",
    "Calculates logistic loss on a binary output. Penalizing _very_ wrong outputs very strongly and not so wrong outputs not so strongly.\n",
    "\n",
    "Loss Function:\n",
    "\n",
    "$J(w) = \\displaystyle\\prod_{i=1}^{m} p(y^i \\mid x^i; w)$\n",
    "\n",
    "We want to maximize the log likelihood. This is also known as the cross entropy function.\n",
    "\n",
    "NOTE: $h(x) = \\frac{1}{1 + e^{-z}}$\n",
    "\n",
    "$log J(w) = \\displaystyle\\sum_{i=1}^{m}{y^i * log h(x)^i + (1 - y^i) log(1 - h(x)^i)} $\n",
    "\n",
    "### Softmax Function\n",
    "\n",
    "Used for classification of _K_ number of classes. All values of the output sum to one. All outputs represent a probability distribution across discrete alternatives.\n",
    "\n",
    "$ P(y = j \\mid x) = \\displaystyle\\frac{e^z_i}{\\sum_{k=1}^{K}{e^z_k}} $\n",
    "\n",
    "$ J(w) = -\\displaystyle\\sum_j{t_j log y_j} $\n",
    "\n",
    "NOTE: lectures didn't show how to change weights\n",
    "\n",
    "\n",
    "* Hessian Multiplicative connections\n",
    "\n",
    "---\n",
    "## Different NN Architectures\n",
    "\n",
    "## Perceptron:\n",
    "\n",
    "> A very simple network architecture. _Features_ are not learned, they're\n",
    "> designed, and weights are learned.\n",
    "\n",
    "* supervised\n",
    "* linear\n",
    "* binary output\n",
    "\n",
    "### Learning Procedure:\n",
    "\n",
    "Guaranteed to work:\n",
    "\n",
    "```\n",
    "foreach(trainingex) {\n",
    "  if output is correct, don't change weights\n",
    "  if output is 0, add input vector to weights\n",
    "  if output is 1, subtract input vector to weights\n",
    "}\n",
    "```\n",
    "\n",
    "[From SO post](https://stats.stackexchange.com/questions/137834/clarification-about-perceptron-rule-vs-gradient-descent-vs-stochastic-gradient)\n",
    "\n",
    "$\\partial L_{\\pmb{w}}(y^{(i)}) = \\begin{array}{rl} \n",
    "\\{ 0 \\},                         &   \\text{ if } y^{(i)} \\pmb{w}^\\top\\pmb{x}^{(i)} > 0 \\\\\n",
    "\\{ -y^{(i)} \\pmb{x}^{(i)} \\},    &   \\text{ if } y^{(i)} \\pmb{w}^\\top\\pmb{x}^{(i)} < 0 \\\\\n",
    "[-1, 0] \\times y^{(i)} \\pmb{x}^{(i)},   &   \\text{ if } \\pmb{w}^\\top\\pmb{x}^{(i)} = 0 \\\\ \n",
    "\\end{array}$\n",
    " \n",
    "Weights from multiple models can be averaged and produce another valid\n",
    "model\n",
    "\n",
    "Can find patterns, but not patterns that \"wrap-around\"\n",
    "\n",
    "## Recurrent Neural Network\n",
    "\n",
    "> Generic structure of NN. Many special case instances follow\n",
    "\n",
    "Good for processing sequences of data, speech and image recognition. Use internal memory.\n",
    "\n",
    "* directed graph\n",
    "* forward prop\n",
    "* back prop\n",
    "\n",
    "Cannot know the hidden states. We could only know a probability\n",
    "distribution of space.\n",
    "\n",
    "### Learning Procedure\n",
    "\n",
    "This is backprop assuming SGD:\n",
    "\n",
    "Inputs are multiplied by weights into a hidden neuron. Hidden neurons are then multiplied by separate weights to produce either more hidden neurons our output neurons. Forward pass complete. Error is computed and then weights in each layer (input => hidden, hidden => output) are updated once more.\n",
    "\n",
    "\n",
    "## LTST memory NN\n",
    "\n",
    "> an implementation of RNN with read gate, write gate and keep gate\n",
    "\n",
    "* does not have vanishing/exploding gradient problem\n",
    "\n",
    "## Feedforward Neural Network\n",
    "\n",
    "\n",
    "## Hopfield NN\n",
    "\n",
    "* special case of RNN without any hidden units"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
